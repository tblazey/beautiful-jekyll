---
layout: post
title: Fitting the Standard DTI Model
category: Python
tags: [Python, DTI]
---

<head>
    <script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>

One of the great things about neuroimaging is the variety of [great](https://surfer.nmr.mgh.harvard.edu/) [software](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki) [packages](https://www.nitrc.org/) for analyzing data. These packages make it fairly easy to perform highly sophisticated and informative analyses. However, the ease of use of these packages does have tend to   seperate the user from the actual computations that take place. While I don't think it necessary for the user to understand every computational detail - I certainly don't - I do think it is important to understand the basics. 

Take, for example, [diffusion weighted imaging](https://en.wikipedia.org/wiki/Diffusion_MRI) (DWI). DWI measures the diffusion of water thoughout the brain, and is used to examine white matter structure. Perhaps the most common model that is used for the analysis of DWI data is the [diffusion tensor model](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2041910/), which  represents the diffusion of water as a 3D ellipsoid. In white matter the diffusion tensor tends to be highly [anisotropic](https://en.wikipedia.org/wiki/Anisotropy) as flow occurs primarily along the axon. A useful way of quantifying anisotropy is [fractional anisotrophy](https://en.wikipedia.org/wiki/Fractional_anisotropy) (FA):

\\[ (1) \quad FA = \sqrt{\frac{3}{2}}\frac{\sqrt{(\lambda_1-\hat{\lambda})^2+(\lambda_2-\hat{\lambda})^2+(\lambda_3-\hat{\lambda})^2}}{\sqrt{\lambda_1^2+\lambda_2^2+\lambda_3^2}} \\]

where \\( \lambda_i \\) is the *i<sup>th</sup>* [eigenvector](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of the diffusion tensor and \\( \hat{\lambda} \\) is the [trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) of the diffusion tensor. 

So how does one obtain the diffusion tensor from DWI images? Well, lucky for us [smart people](https://www.ncbi.nlm.nih.gov/pubmed/16828568) have already figured that out for us. The relevant equation is:

\\[ (2) \quad S_i = S_0\exp(-\textit{b}_i\textbf{g}_i^T\textbf{Dg}_i) \\]

where \\( S_i \\) is the *i<sup>th</sup>* diffusion weighted image, \\( S_0 \\) is the signal from the image(s) without any diffusion weighting, \\( \textit{b}_i \\) is the diffusion weighting (commonly refered to as b-values), and \\( \textit{g}_i \\) is a vector containing the direction of the *i<sup>th</sup>* diffusion gradient (b-vectors).

Perhaps the most straightfoward way of obtaining the diffusion tensor is to directly fit equation 2 using nonlinear least squres. Fortunately, the [Scipy's optimization library](https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html) makes this pretty easy. First, we need to create a function that returns the model predictions given a specific diffusion tensor:

{% highlight Python %}
#Tensor fitting function
def tensorCalc(X,sZero,xx,xy,xz,yy,yz,zz):

	#Extract components of X
	bvecs = X[:,0:3]
	bvals = X[:,3]
		
	#Construct tensor
	D = np.array([[xx,xy,xz],[xy,yy,yz],[xz,yz,zz]])
		
	#Calculate model fit for each image
	nImg = X.shape[0]; pred = np.zeros(nImg)
	for imgIdx in range(nImg):
		pred[imgIdx] =  sZero*np.exp(-bvals[imgIdx]*(np.dot(bvecs[imgIdx,:].T,D).dot(bvecs[imgIdx,:])))
		
	#Return model predictions
	return pred
{% endhighlight %}

Next, we need to get some DWI data. For this example I downloaded the [tutorial data](http://camino.cs.ucl.ac.uk/index.php?n=Tutorials.DTI#example_human_data) from [Camino](http://camino.cs.ucl.ac.uk/index.php), which is a great DWI analysis package. Once downloaded, you can easily load the data into Python:

{% highlight Python %}
#Load in b-vectors
bvecs = np.loadtxt('grad_dirs.txt')

#Construct b-values
nDwi = bvecs.shape[0]
bvals = np.repeat(1000,nDwi); bvals[bvecs[:,0]==0] = 0

#Load in DWI image
dwi = nib.load('4Ddwi_b1000.nii.gz')
dwiData = dwi.get_data()
{% endhighlight %}

With that we have what we need to fit the diffusion tensor. We can simply call scipy.optimize.curvefit:
{% highlight Python %}
#Run fit
fit,cov = opt.curve_fit(tensorCalc,np.hstack((bvecs,bvals[:,np.newaxis])),
			dwiData[50,50,25,:],p0=np.hstack((np.mean(vox[bvals==0]),np.repeat(1E-2,6))))
			
#Reconstruct tensor
D = np.array([[fit[1],fit[2],fit[3]],[fit[2],fit[4],fit[5]],[fit[3],fit[5],fit[6]]])

#Calculate FA
fa = np.sqrt(np.power(S[0]-meanD,2) + np.power(S[1]-meanD,2) + np.power(S[2]-meanD,2)) / np.sqrt(np.power(S[0],2)+np.power(S[1],2)+np.power(S[2],2)) * np.sqrt(3.0/2.0)
{% endhighlight %}

which gives the following tensor and FA values:

\\[ 
    D = \begin{bmatrix}
    6.53\times10^{-4} & 2.99\times10^{-5} & 1.20\times10^{-4} \\\
    2.99\times10^{-5} & 1.15\times10^{-3} & 1.92\times10^{-4} \\\
    1.20\times10^{-4} & 1.92\times10^{-4} & 4.95\times10^{-4}
	\end{bmatrix} 
\\]

\\[ FA = 0.491 \\]

While nonlinear least squares works fine, there is a simplier linear formulation of the diffusion model that is often used. To get this formulation, we first take the logarithm of equation 2:

\\[ (3) \quad \log(S_i) = \log(S_0) + -\textit{b}_i\textbf{g}_i^T\textbf{Dg}_i \\]

You can then multiply out the terms on the right hand side to get the equation into a standard linear regression format:

\\[ (4) \quad y= X\beta \\]

where:

\\[ y = \begin{bmatrix}
	\log(S_1) \\\
	\log(S_2) \\\
	\vdots \\\
	\log(S_n)
	\end{bmatrix}, \quad 
	X = \begin{bmatrix}
	1 & -b_1g_x_1^2
	\end{bmatrix}
\\]


